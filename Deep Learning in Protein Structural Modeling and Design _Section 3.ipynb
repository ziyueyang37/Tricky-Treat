{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning in Protein Structral Modeling and Design Section 3\n",
    "08/14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/CNN.gif\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From CNN (Convolutional neural network) to GCN (Graph convolutional network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN\n",
    "<img src=\"img/CNN.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GCN\n",
    "<img src=\"img/GCN.jpg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resnet\n",
    "<img src=\"img/resnet.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Difficulties in Deep Neural Network / How does Resnet Help?\n",
    "1. Vanishing Gradient / Exploding Gradient (Accumulate through many layers)    \n",
    "2. Network Degradation (Not good at doing identity projection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Recurrent Neural Network (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"img/rnn.jpg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM\n",
    "<img src=\"img/RNN.gif\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. forget gate \n",
    "Control how much to forget from previous step memory state\n",
    "<img src=\"img/lstm-forget.jpg\" width=\"400\">\n",
    "2. input gatem\n",
    "Control how much new calculated state are updated to memory state\n",
    "<img src=\"img/lstm-input.jpg\" width=\"400\">\n",
    "cell state C\n",
    "<img src=\"img/lstm-memory.jpg\" width=\"400\">\n",
    "3. output gate\n",
    "<img src=\"img/lstm-output.jpg\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU (Gated Recurrent Units)\n",
    "<img src=\"img/GRU.jpg\" width=\"600\">\n",
    "\n",
    "1. Reset gates help capture short-term dependencies in time series.\n",
    "\n",
    "2. Update gates help capture long-term dependencies in time series.\n",
    "\n",
    "r decides the influence of previous hidden state h(t-1) to new memory H\n",
    "\n",
    "H is combination of input x(t) and h(t-1)\n",
    "\n",
    "z decides how much h(t-1) pass to h(t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference\n",
    "1. GRU is more likely to convergence, less parameters.\n",
    "2. When huge dataset, LSTM performs better since it is more complex\n",
    "3. GRU has two gates (update, reset), LSTM has three gates (forget, input, output)\n",
    "4. GRU pass the hidden state directly to the next unit, while LSTM pack the hidden state using memory cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Variational Auto-encoder (VAE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auto-Encoder\n",
    "<img src=\"img/AE.jpg\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variational Auto-Encoder\n",
    "<img src=\"img/VAE2.png\" width=\"600\">\n",
    "<img src=\"img/VAE_loss.jpg\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variational Inference\n",
    "Any procedure which uses optimization to approximate a density can be termed ``variational inference''.\n",
    "<img src=\"img/VIII.jpg\" width=\"400\">\n",
    "<img src=\"img/VI.jpg\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reparameterization trick\n",
    "\n",
    "<img src=\"img/VII.jpg\" width=\"500\">\n",
    "\\begin{align}\n",
    "Z & = \\mu + \\sigma * \\epsilon\\\\\n",
    "\\end{align}\n",
    "Instead of having a full stochastic node that is blocking all the gradients because we cannot do backprop through it, we are going to split it up into a part where we can do back propagation and the other part which is still stochastic but we do not need to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Generative Adversarial Network (GAN)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/GAN.jpg\" width=\"500\">\n",
    "<img src=\"img/GAN_loss.jpg\" width=\"500\">\n",
    "<img src=\"img/GAN_train.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WGAN (Wasserstein GAN)\n",
    "<img src=\"img/WGAN.jpg\" width=\"500\">\n",
    "\n",
    "1. Remove sigmoid function from the last layer\n",
    "\n",
    "2. Do not take log in the loss function of D and G\n",
    "\n",
    "3. Set a upper and lower bound for the parameters (weights) in D (constant c)\n",
    "\n",
    "4. Do not use momentum/Adam optimizer (which is based on momentum), can use RMSProp or SGD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wasserstein Distance\n",
    "<img src=\"img/KL.jpg\" width=\"300\">\n",
    "<img src=\"img/JS.jpg\" width=\"500\">\n",
    "When D is too good (D(x)=1)\n",
    "<img src=\"img/bzd.jpg\" width=\"400\">\n",
    "\n",
    "<img src=\"img/bzd1.jpg\" width=\"200\">\n",
    "\n",
    "1. P1(x) = 0 and P2(x) != 0\n",
    "2. P1(x) = 0 and P2(x) = 0\n",
    "3. P1(x) != 0 and P2(x) != 0\n",
    "4. P1(x) != 0 and P2(x) = 0\n",
    "<img src=\"img/log2.jpg\" width=\"200\">\n",
    "The discriminator cannot be too good.\n",
    "\n",
    "When no overlap or overlap can be ignored between P1 and P2, JS divergence equals 0 or log2, no gradient\n",
    "<img src=\"img/ganloss.jpg\" width=\"200\">\n",
    "<img src=\"img/kljs.jpg\" width=\"200\">\n",
    "\n",
    "1. Gradient unstable, optimization target is wierd\n",
    "\n",
    "2. Uneven penalty since KL divergence is unsymmetric => collapse mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/wdistance.jpg\" width=\"300\">\n",
    "<img src=\"img/w.jpg\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
